\input{../../slides_preamb.tex}


\subtitle{Dynamic Programming}

\author{John Stachurski}

\date{September 2022}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\section{Introduction}


\begin{frame}
    \frametitle{Introduction}

    Summary of this lecture:

    \begin{itemize}
        \item Foobar
            \vspace{0.3em}
        \item Foobar
    \end{itemize}

            \vspace{0.3em}
            \vspace{0.3em}

    %\emp{Lecture slides at} \url{https://github.com/jstac/tokyo_2022_coursework}

\end{frame}





\begin{frame}
    \frametitle{Introduction to Dynamic Programming}

    Dynamic program

    \begin{algorithm}[H]
      \DontPrintSemicolon
      an initial state $X_0$ is given \;
      $t \leftarrow 0$ \; %  \tcp{foo}
      \While{$t < T$}
      {
          observe current state $X_t$   \;
          choose action $A_t$ \;
          receive reward $R_t$ based on $(X_t, A_t)$ \;
          state updates to $X_{t+1}$ \;
          $t \leftarrow t + 1$ \;
      }
    \end{algorithm}

\end{frame}

\begin{frame}
    
    \begin{figure}
        \centering
        \vspace{1em}
        \scalebox{0.4}{\input{local_figs/state_action_reward.pdf_t}}
        \vspace{1em}
        \caption{\label{f:state_action_reward} A dynamic program}
    \end{figure}

\end{frame}


\begin{frame}
    
    Comments:
    %
    \begin{itemize}
        \item Objective: maximize \emp{lifetime rewards}
            \vspace{0.3em}
            \vspace{0.3em}
        \begin{itemize}
            \item Some aggregation of $R_0, R_1, \ldots$
            \vspace{0.3em}
            \vspace{0.3em}
            \item \Eg $\EE [ R_0 + \beta R_1 + \beta^2 R_2 + \cdots]$ for
                some $\beta \in (0, 1)$
        \end{itemize}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
        \item If $T < \infty$ then the problem is called a \navy{finite horizon} problem  
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
        \item Otherwise it is called an \navy{infinite horizon} problem
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
        \item The update rule can also depend on random elements:
            %
            \begin{equation*}
                X_{t+1} = F(X_t, A_t, \xi_{t+1})
            \end{equation*}
            %
    \end{itemize}

\end{frame}


\begin{frame}

    \Eg A retailer sets prices and manages inventories to maximize profits

    \begin{itemize}
        \item $X_t$ measures 
            \begin{itemize}
                \item current business environment
            \vspace{0.3em}
                \item the size of the inventories
            \vspace{0.3em}
                \item prices set by competitors, etc. 
            \end{itemize}
            \vspace{0.3em}
        \item $A_t$ specifies current prices and orders of new stock
            \vspace{0.3em}
            \vspace{0.3em}
        \item $R_t$ is current profit $\pi_t$
            \vspace{0.3em}
            \vspace{0.3em}
        \item Lifetime reward is 
            %
            \begin{equation*}
                \EE \left[ \pi_0 + \frac{1}{1+r} \pi_1 
                    + \left(\frac{1}{1+r}\right)^2 \pi_2
                    + \cdots \right]
                    = \text{EPV}
            \end{equation*}
    \end{itemize}


\end{frame}



\begin{frame}
    \frametitle{Markov Decision Processes}    


    \begin{itemize}
        \item A class of dynamic programs
    \vspace{0.5em}
        \item Broad enough to encompass many economic applications
    \vspace{0.5em}
        \item Includes optimal stopping problems as a special case
    \vspace{0.5em}
        \item Clean, powerful theory
    \vspace{0.5em}
        \item A range of important algorithms 
    \end{itemize}

    \vspace{0.5em}
    Also a cornerstone for
    %
    \begin{itemize}
        \item reinforcement learning, artificial intelligence, etc.
    \end{itemize}

\end{frame}


\begin{frame}
    
    MDPs are dynamic programs characterized by two features
    %
    \begin{enumerate}
        \item Rewards are additively separable:
            %
            \begin{equation*}
                \text{lifetime reward }
                = \EE \sum_{t \geq 0} \beta^t R_t
            \end{equation*}
        \item The discount rate is constant
    \end{enumerate}

    \vspace{0.5em}
    \vspace{0.5em}
    For now we restrict attention to \underline{finite} state and action spaces

    \vspace{0.5em}
    \begin{itemize}
        \item Routinely used in quantitative applications
    \vspace{0.5em}
        \item Avoids technical issues we can put aside for later
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Notation}

    Let $\Xsf$ and $\Asf$ be any sets
    
    \vspace{0.5em}
    A \navy{correspondence} $\Gamma$ from $\Xsf$ to $\Asf$
    is a map that associates each $x \in \Xsf$ to a subset of $\Asf$
    
    \vspace{0.5em}
    \begin{itemize}
        \item called \navy{nonempty} if $\Gamma(x) \not= \emptyset$ for all $x
            \in \Xsf$
    \end{itemize}
    \vspace{0.5em}
    \vspace{0.5em}

    \Egs

    \begin{itemize}
        \item $\Gamma(x) = [0, x]$ is a correspondence from $\RR$ to $\RR$
    \vspace{0.5em}
        \item $\Gamma(x) = [-x, x]$ is a nonempty correspondence from $\RR$ to $\RR$
    \end{itemize}

\end{frame}



\begin{frame}
    
    \vspace{0.5em}
    We study a controller who, at each integer $t \geq 0$
    %
    \begin{enumerate}
        \item observes the current state $X_t$
    \vspace{0.5em}
        \item responds with an action $A_t$
    \end{enumerate}

    \vspace{0.5em}
    Her aim is to maximize expected discounted
    rewards
    %
    \begin{equation*}\label{eq:dmdp_ob}
        \EE \sum_{t \geq 0} \beta^t r(X_t, A_t),
        \qquad X_0 = x_0 \text{ given}
    \end{equation*}
    %

    \vspace{0.5em}
    We take as given 
    %
    \begin{enumerate}
        \item a finite set $\Xsf$ called the \navy{state space} and
    \vspace{0.5em}
        \item a finite set $\Asf$ called the \navy{action space}
    \end{enumerate}


\end{frame}

\begin{frame}

    The actions of the controller are limited 
    by a \navy{feasible correspondence} $\Gamma$

    \begin{itemize}
        \item A correspondence from $\Xsf$ to $\Asf$
    \vspace{0.5em}
        \item $\Gamma(x)$ is the set of actions available to the controller in
            state $x$
    \end{itemize}

    \vspace{0.5em}
    \vspace{0.5em}

    Given $\Gamma$, we set
    %
    $$\Gsf := \setntn{(x, a) \in \Xsf \times \Asf}{a \in \Gamma(x)}$$

    \vspace{0.5em}
    \begin{itemize}
        \item called the set of \navy{feasible state-action pairs}
    \end{itemize}


    \vspace{0.5em}
    \vspace{0.5em}
    Reward $r(x, a)$ is received at feasible state-action pair $(x, a)$, 

\end{frame}


\begin{frame}



    A \navy{stochastic kernel} from $\Gsf$ to $\Xsf$ is a map 
    $P \colon \Gsf \times \Xsf \to \RR_+$
            satisfying
            %
            \begin{equation*}
                \sum_{x' \in \Xsf} P(x, a, x') = 1
                \quad \text{ for all $(x,a)$ in $\Gsf$} 
            \end{equation*}
            %

    \vspace{0.5em}
    \vspace{0.5em}
    Interpretation
    %
    \begin{itemize}
        \item For each feasible state-action pair, $P(x,a, \cdot)$ is a
            distribution
    \vspace{0.5em}
        \item The next period state $x'$ is selected from $P(x,a, \cdot)$
    \end{itemize}


    \vspace{0.5em}
    \vspace{0.5em}
    \vspace{0.5em}

    Now let's put it all together:

\end{frame}


\begin{frame}

    Given $\Xsf$ and $\Asf$, a \navy{Markov decision process}
     (\navy{MDP}) is a tuple $(\Gamma, \beta, r, P)$ where
    %
    \begin{enumerate}
        \item $\Gamma$ is a nonempty correspondence from $\Xsf \to \Asf$
    \vspace{0.5em}
        \item $\beta$ is a constant in $(0, 1)$
    \vspace{0.5em}
        \item $r$ is a function from $\Gsf$ to $\RR$
    \vspace{0.5em}
        \item $P$ is a stochastic kernel from $\Gsf$
            to $\Xsf$
    \vspace{0.5em}
    \end{enumerate}
       
    \vspace{0.5em}
    In the foregoing,
    %
    \begin{itemize}
        \item $\beta$ is called the \navy{discount factor}
    \vspace{0.5em}
        \item $r$ is called the \navy{reward function}
    \end{itemize}

\end{frame}


\begin{frame}
    
    {\small
    \begin{algorithm}[H]
    \DontPrintSemicolon
    $t \leftarrow 0$ \;
    input $X_0$ \;
    \While{$t < \infty$}
    {
        observe $X_t$   \;
        choose action $A_t$ from $\Gamma(X_t)$  \;
        receive reward $r(X_t, A_t)$   \;
        draw $X_{t+1}$ from $P(X_t, A_t, \cdot)$   \;
        $t \leftarrow t + 1$ \;
    }
    \caption{\label{algo:d_mdp} MDP dynamics: states, actions, and rewards}
    \end{algorithm}
    }

    \vspace{0.5em}
    Rules:
    %
    \begin{itemize}
        \item Choose $(A_t)_{t \geq 0}$ to maximize $\EE \sum_{t \geq 0}
            \beta^t r(X_t, A_t)$
        \item Actions don't depend on future outcomes 
    \end{itemize}

\end{frame}



\begin{frame}
    
    The \navy{Bellman equation} is
    %
    \begin{equation*}
            v(x)
            = \max_{a \in \Gamma(x)}
            \left\{
                r(x, a)
                + \beta
                \sum_{x' \in \Xsf} v(x') P(x, a, x')
            \right\}
    \end{equation*}
    %


    Reduces an infinite horizon problem to a two period
    problem

    In the two period problem, the controller trades off
    %
    \begin{enumerate}
        \item current rewards and 
        \item expected discounted value from future states
    \end{enumerate}

    Current actions influence both terms

\end{frame}


\begin{frame}
    
    ADD inventory example

\end{frame}


\begin{frame}
    \frametitle{Policies}
    
    Actions will be governed by policies
    %
    \begin{itemize}
        \item maps from states to actions
        \vspace{0.5em}
        \item today's action is a function of today's state!
    \end{itemize}

    \vspace{0.5em}
    The set of \navy{feasible policies} is
    %
    \begin{equation*}
        \Sigma := 
        \text{ all }
            \sigma \in \Asf^\Xsf
            \st 
            \sigma(x) \in \Gamma(x) \text{ for all } x \in \Xsf
    \end{equation*}
    %

    \vspace{0.5em}
    Meaning of selecting $\sigma$ from $\Sigma$: 

    \begin{itemize}
        \item respond to state $X_t$ with action $A_t := \sigma(X_t)$ at all $t$
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Dynamics}

    What happens when we always follow $\sigma \in \Sigma$?
    
    \vspace{0.5em}
    Now 
    %
    \begin{equation*}
        X_{t+1} \sim P(X_t, \sigma(X_t), \cdot)
        \quad \text{at every $t$}
    \end{equation*}

    \vspace{0.5em}
    Thus, $X_t$ updates according to the stochastic matrix 
    %
    \begin{equation*}
        P_\sigma(x, x') := P(x, \sigma(x), x')
        \qquad (x, x' \in \Xsf)
    \end{equation*}
    %

    \vspace{0.5em}
    The state process becomes $P_\sigma$-Markov

    \begin{itemize}
        \item Fixing a policy ``closes the loop'' in the state dynamics
        \vspace{0.5em}
        \item Solving an MDP means choosing a Markov chain!
    \end{itemize}

\end{frame}


\begin{frame}
    \frametitle{Rewards}
    
    Under the policy $\sigma$, rewards at $x$ are $r(x, \sigma(x))$  

    \vspace{0.5em}
    Let
    %
    \begin{equation*}
        r_\sigma(x) := r(x, \sigma(x))
        \qquad (x \in \Xsf)
    \end{equation*}
    %

    \vspace{0.5em}
    Now set 
    %
    $$\EE_x := \EE[ \; \cdot \; \given X_0 = x]$$

    \vspace{0.5em}
    Then the expected time $t$ reward is
    %
    \begin{equation*}
        \EE_x  \, r(X_t, A_t) 
        = \EE_x  \, r_\sigma(X_t)  
        = (P_\sigma^t \, r_\sigma) (x)
    \end{equation*}
    %

\end{frame}



\begin{frame}

    Let $(X_t)_{t \geq 0}$ be $P_\sigma$-Markov with $X_0 = x$
    
    \vspace{0.5em}
    \vspace{0.5em}
    The lifetime value of $\sigma$ starting from $x$ is
    %
    \begin{equation*}
        v_\sigma (x) 
        := \EE_x \sum_{t \geq 0} \beta^t
            r_\sigma(X_t) 
    \end{equation*}
    %


    \vspace{0.5em}
    Since $\beta < 1$, we have $r(\beta P_\sigma) <1$ and hence
    %
    \begin{equation*}
        v_\sigma 
        = \sum_{t \geq 0} \beta^t P_\sigma^t \, r_\sigma 
        = (I - \beta P_\sigma)^{-1} \, r_\sigma 
    \end{equation*}
    %

    \vspace{0.5em}
    The \navy{value function}\index{Value function} is defined as 
    %
    \begin{equation*}
        v^*(x) = \max_{\sigma \in \Sigma} v_\sigma(x)
        \qquad (x \in \Xsf)
    \end{equation*}
    %

\end{frame}


\begin{frame}
    
    Recall that the Bellman equation is 
    %
    \begin{equation*}
            v(x)
            = \max_{a \in \Gamma(x)}
            \left\{
                r(x, a)
                + \beta
                \sum_{x' \in \Xsf} v(x') P(x, a, x')
            \right\}
    \end{equation*}
    %

    \vspace{0.5em}
    The \navy{Bellman operator} for the MDP
    is the self-map $T$ on $\RR^\Xsf$ defined by
    %
    \begin{equation*}
        (Tv)(x)
            = \max_{a \in \Gamma(x)}
            \left\{
                r(x, a)
                + \beta
                \sum_{x' \in \Xsf} v(x') P(x, a, x')
            \right\}
    \end{equation*}
    %

    Obviously 
    %
    \begin{itemize}
        \item $Tv=v$ iff $v$ satisfies the Bellman equation
    \vspace{0.5em}
        \item $T$ is order-preserving on $\RR^\Xsf$
    \end{itemize}

\end{frame}



\begin{frame}
    
    Fix $v \in \RR^\Xsf$

    \vspace{0.5em}
    A policy $\sigma \in \Sigma$ is called \navy{$v$-greedy} if
    %
    \begin{equation*}
        \forall \; x \in \Xsf, \;\;
        \sigma(x)
        \in \argmax_{a \in \Gamma(x)}
        \left\{
            r(x, a)
            + \beta
            \sum_{x' \in \Xsf} v(x') P(x, a, x')
        \right\}
    \end{equation*}
    %

    \vspace{0.5em}
    A policy $\sigma \in \Sigma$ is called \navy{optimal} if
    %
    \begin{equation*}
        v_\sigma = v^*
    \end{equation*}

    Thus,
    %
    \begin{center}
        $\sigma$ is optimal $\iff$ lifetime value is maximal at each state  
    \end{center}


\end{frame}

\begin{frame}
    
    {\bf Proposition.} For the MDP described above
    %
    \begin{enumerate}
        \item $v^*$ is the unique fixed point of $T$
            in $\RR^\Xsf$
    \vspace{0.5em}
        \item $T$ is a contraction of modulus $\beta$ on $\RR^\Xsf$ under the
            norm $\| \cdot \|_\infty$ 
    \vspace{0.5em}
        \item A feasible policy is optimal if and only it is
            $v^*$-greedy
    \vspace{0.5em}
        \item At least one optimal policy exists
    \end{enumerate}
    %


    \vspace{0.5em}
    \vspace{0.5em}
    Proof: 
    %
    \begin{itemize}
        \item similar to that for optimal stopping
    \vspace{0.5em}
        \item full details deferred until we study RDPs
    \end{itemize}


\end{frame}



\end{document}









