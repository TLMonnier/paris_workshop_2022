\input{../../slides_preamb.tex}


\subtitle{Dynamic Programming}

\author{John Stachurski}

\date{September 2022}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}


\section{Introduction}


\begin{frame}
    \frametitle{Introduction}

    Summary of this lecture:

    \begin{itemize}
        \item Introduce dynamic programming
            \vspace{0.3em}
            \vspace{0.3em}
        \item Introduce the RDP framework and provide examples
            \vspace{0.3em}
            \vspace{0.3em}
            \begin{itemize}
                \item inventories, optimal savings, job search
            \end{itemize}
            \vspace{0.3em}
            \vspace{0.3em}
        \item Provide RDP optimality results
            \vspace{0.3em}
            \vspace{0.3em}
        \item Discuss algorithms
            \vspace{0.3em}
            \vspace{0.3em}
        \item Show algorithms and their performance for some case
            \vspace{0.3em}
            \vspace{0.3em}
            \begin{itemize}
                \item optimal savings, optimal investment...
            \end{itemize}
    \end{itemize}

            \vspace{0.3em}
            \vspace{0.3em}

    %\emp{Lecture slides at} \url{https://github.com/jstac/tokyo_2022_coursework}

\end{frame}





\begin{frame}
    \frametitle{Introduction to Dynamic Programming}

    Dynamic program

    \begin{algorithm}[H]
      \DontPrintSemicolon
      an initial state $X_0$ is given \;
      $t \leftarrow 0$ \; %  \tcp{foo}
      \While{$t < T$}
      {
          observe current state $X_t$   \;
          choose action $A_t$ \;
          receive reward $R_t$ based on $(X_t, A_t)$ \;
          state updates to $X_{t+1}$ \;
          $t \leftarrow t + 1$ \;
      }
    \end{algorithm}

\end{frame}

\begin{frame}
    
    \begin{figure}
        \centering
        \vspace{1em}
        \scalebox{0.4}{\input{local_figs/state_action_reward.pdf_t}}
        \vspace{1em}
        \caption{\label{f:state_action_reward} A dynamic program}
    \end{figure}

\end{frame}


\begin{frame}
    
    Comments:
    %
    \begin{itemize}
        \item Objective: maximize \emp{lifetime rewards}
            \vspace{0.3em}
            \vspace{0.3em}
        \begin{itemize}
            \vspace{0.3em}
            \vspace{0.3em}
            \item \Eg $\EE [ R_0 + \beta R_1 + \beta^2 R_2 + \cdots]$ for
                some $\beta \in (0, 1)$
        \end{itemize}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
        \item If $T < \infty$ then the problem is called a \navy{finite horizon} problem  
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
        \item Otherwise it is called an \navy{infinite horizon} problem
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
            \vspace{0.3em}
        \item The update rule can also depend on random elements:
            %
            \begin{equation*}
                X_{t+1} = F(X_t, A_t, \xi_{t+1})
            \end{equation*}
            %
    \end{itemize}

\end{frame}


%\begin{frame}

    %\Eg A retailer sets prices and manages inventories to maximize profits

    %\begin{itemize}
        %\item $X_t$ measures 
            %\begin{itemize}
                %\item current business environment
            %\vspace{0.3em}
                %\item the size of the inventories
            %\vspace{0.3em}
                %\item prices set by competitors, etc. 
            %\end{itemize}
            %\vspace{0.3em}
        %\item $A_t$ specifies current prices and orders of new stock
            %\vspace{0.3em}
            %\vspace{0.3em}
        %\item $R_t$ is current profit $\pi_t$
            %\vspace{0.3em}
            %\vspace{0.3em}
        %\item Lifetime reward is 
            %%
            %\begin{equation*}
                %\EE \left[ \pi_0 + \frac{1}{1+r} \pi_1 
                    %+ \left(\frac{1}{1+r}\right)^2 \pi_2
                    %+ \cdots \right]
                    %= \text{EPV}
            %\end{equation*}
    %\end{itemize}


%\end{frame}



\begin{frame}
    \frametitle{Example: Optimal Inventories}

    Given a demand process $(D_t)_{t \geq 0}$, inventory $(X_t)_{t \geq 0}$
    obeys
    %
    \begin{equation*}
        X_{t+1} = F(X_t, A_t, D_{t+1}) 
    \end{equation*}
    %
    where

    \begin{itemize}
        \item $A_t$ is units of stock ordered this period 
        \vspace{0.5em}
        \item $F(x, a, d) := \max\{x - d, 0\} + a$
    \end{itemize}

    \vspace{0.5em}
    The firm can store at most $K$ items at one time

    \vspace{0.5em}
    \begin{itemize}
        \item The state space is $\Xsf := \{0, \ldots, K\}$
    \end{itemize}

    \vspace{0.5em}
    We assume $(D_t) \iidsim \phi \in \dD(\ZZ+)$  

\end{frame}

\begin{frame}
    
    Profits are given by
    %
    \begin{equation*}
        \pi_t := X_t \wedge D_{t+1} - c A_t - \kappa \1\{A_t > 0\}
    \end{equation*}
    %

    \begin{itemize}
        \item Orders in excess of
            inventory are lost 
    \vspace{0.5em}
        \item $c$ is unit product cost (and unit sales prices $=1$)
    \vspace{0.5em}
        \item $\kappa$ is a fixed cost of ordering inventory
    \end{itemize}

    \vspace{0.5em}
    \vspace{0.5em}

    With $\beta := 1/(1+r)$ and $r > 0$, the value of the firm is 
    %
    \begin{equation*}
        V_0 = \EE \sum_{t \geq 0} \beta^t \pi_t
    \end{equation*}
    %

    \vspace{0.5em}
    \vspace{0.5em}
    Objective: maximize (shareholder) value

\end{frame}


\begin{frame}
    
    Expected current profit is
    %
    \begin{equation*}
        r(x, a)  := \sum_{d \geq 0} (x \wedge d) \phi(d) 
            - c a - \kappa \1\{a > 0\}
    \end{equation*}

    \vspace{0.5em}
    \vspace{0.5em}
    The set of feasible order sizes
    at $x$ is 
    %
    \begin{equation*}
        \Gamma(x) := \{0, \ldots, K - x\}
    \end{equation*}
    %
    \vspace{-1.5em}
    \begin{itemize}
        \item $\Gamma$ is the \navy{feasible correspondence}
    \end{itemize}

    \vspace{0.5em}
    The \navy{Bellman equation} is
    %
    \begin{equation*}
        v(x)
        = \max_{a \in \Gamma(x)} 
        \left\{
            r(x, a)
            + \beta
            \sum_{d \geq 0} F(x, a, d) \phi(d)
        \right\}
    \end{equation*}

\end{frame}

\begin{frame}
    
    The \emp{standard solution procedure} for this problem is VFI:

    %
    \begin{enumerate}
        \item define the Bellman operator $T$ via
            %
            \begin{equation*}
                (Tv)(x)
                = \max_{a \in \Gamma(x)} 
                \left\{
                    r(x, a)
                    + \beta
                    \sum_{d } v(F(x,a,d)) \phi(d)
                \right\}
            \end{equation*}
            \vspace{0.5em}
        \item iterate with $T$ to (approximately) compute the
            fixed point $v^*$ and
            \vspace{0.5em}
        \item compute a $v^*$-greedy greedy policy $\sigma^*$, which satisfies
            %
            \begin{equation*}
                \sigma^*(x)
                \in \argmax_{a \in \Gamma(x)} 
                \left\{
                    r(x, a)
                    + \beta
                    \sum_{d } v^*(F(x,a,d)) \phi(d)
                \right\}
            \end{equation*}
    \end{enumerate}

\end{frame}



\begin{frame}
    \frametitle{Optimal Savings with Labor Income}

    
    Wealth evolves according to
    %
    \begin{equation*}
        C_t + W_{t+1} \leq R W_t + Y_t 
        \qquad (t = 0, 1, \ldots)
    \end{equation*}
    %

        \vspace{0.5em}
    \begin{itemize}
        \item $(W_t)$ takes values in finite set $\Wsf \subset \RR_+$ 
        \vspace{0.5em}
        \item $(Y_t)$ is $Q$-Markov chain on finite set $\Ysf$ 
        \vspace{0.5em}
        \item $C_t \geq 0$
    \end{itemize}

        \vspace{0.5em}
    The household maximizes

    %
    \begin{equation*}
        \EE \sum_{t \geq 0} \beta^t u(C_t)
    \end{equation*}


\end{frame}

\begin{frame}
    
    The Bellman equation is
    %
    \begin{multline*}
        v(w, y) = 
        \\
        \max_{w' \in \Gamma(w, y)}
        \left\{
            u(R w + y - w')
            + \beta \sum_{y' \in \Ysf} v(w', y') Q(y, y')
        \right\}
    \end{multline*}
    %

        \vspace{0.5em}
        \vspace{0.5em}
    The standard solution procedure is VFI

    \begin{enumerate}
        \item Set up Bellman operator $T$
        \vspace{0.5em}
        \item Iterate with $T$ from some initial guess to approximate $v^*$
        \vspace{0.5em}
        \item Compute the $v^*$-greedy policy
    \end{enumerate}

\end{frame}



\begin{frame}
    \frametitle{Recursive Decision Processes}

    We will study an abstract dynamic program with Bellman equation
    %
    \begin{equation*}
        v(x) = \max_{a \in \Gamma(x)} B(x, a, v)
    \end{equation*}

        \vspace{0.5em}
        \vspace{0.5em}
    Advantages of ``abstract'' dynamic programming

    \begin{itemize}
        \item Subsumes standard Markov decision processes
        \vspace{0.5em}
        \item Can handle state-dependent discounting, recursive prefs, etc.
        \vspace{0.5em}
        \item Abstraction means clean proofs
        \vspace{0.5em}
        \item Abstraction allows better analysis of algorithms
    \end{itemize}

\end{frame}


\begin{frame}
    
    Let $\Xsf$ and $\Asf$ be finite sets (\navy{state} and \navy{action spaces})

        \vspace{0.5em}
    Actions are constrained by 
        a nonempty correspondence $\Gamma$ from $\Xsf$ to $\Asf$ called
            the \navy{feasible correspondence}


    The feasible correspondence in turn defines
    %
    \begin{enumerate}
        \item the \navy{feasible state-action pairs}
            %
            \begin{equation*}
                \Gsf := \setntn{(x, a) \in \Xsf \times \Asf}{a \in \Gamma(x)}
            \end{equation*}
            %
        \item the set of \navy{feasible policies} 
            %
            \begin{equation*}
                \Sigma := 
                    \setntn{\sigma \in \Asf^\Xsf}
                    {\sigma(x) \in \Gamma(x) \text{ for all } x \in \Xsf}.
            \end{equation*}
            %
    \end{enumerate}

    \begin{itemize}
        \item ``follow'' $\sigma$ $\iff$ always respond to state $x$ with action
            $\sigma(x)$
    \end{itemize}

\end{frame}

\begin{frame}
    
    Given $\Xsf$, $\Asf$ and $\Gamma$, a \navy{recursive
    decision process} (RDP) consists of
    %
    
    \begin{enumerate}
        \item a closed subset $\vV$ of $\RR^\Xsf$ called the set of
            \navy{candidate value functions} and
        \vspace{0.5em}
        \item a  \navy{value aggregator}, which is a function
                %
                \begin{equation*}
                    B \colon \Gsf \times \vV \to \RR
                \end{equation*}
                %
              satisfying $v, w \in \vV$ and $v \leq w \implies$
              %
              \vspace{0.5em}
              \begin{equation*}
                B(x, a, v) \leq B(x, a, w) \; \text{ for all $(x, a) \in \Gsf$}
              \end{equation*}
                %
              and 
                %
                \begin{equation*}
                    \sigma \in \Sigma 
                    \, \text{ and } \,
                    v \in \vV
                    \; \implies \;
                    w \in \vV
                    \quad \text{ where }
                    w(x) := B(x, \sigma(x), v)
                \end{equation*}
                %
    \end{enumerate}

\end{frame}


\begin{frame}
    

    \Eg For the inventory problem we set 
    %
    \begin{itemize}
        \item $\Gamma(x) := \{0, \ldots, K - x\}$,
        \item $\vV = \RR^\Xsf$ and
        %
        \begin{equation*}
            B(x, a, v) :=
            \left\{
                r(x, a)
                + \beta
                \sum_{d \geq 0} v(F(x,a,d)) \phi(d)
            \right\}
        \end{equation*}
    \end{itemize}

    The Bellman equation is then 
    %
    $$v(x) = \max_{a \in \Gamma(x)} B(x, a, v)$$

    The function $B$ is a valid value aggergator

    For example, if $v \leq w$, then
    %
    \begin{equation*}
        B(x, a, v) \leq B(x, a, w)
    \end{equation*}

\end{frame}


\begin{frame}
    

    \Eg For the savings problem we set 
    %
    \begin{itemize}
        \item $\Gamma(w, y) := \{w' \in \Wsf \,:\, w' \leq Rw + y\}$,
        \item $\vV = \RR^\Xsf$ and
        %
        \begin{equation*}
            B((w,y), w', v) :=
            u(R w + y - w')
            + \beta \sum_{y' \in \Ysf} v(w', y') Q(y, y')
        \end{equation*}
    \end{itemize}

    The Bellman equation is then 
    %
    $$v(x) = \max_{a \in \Gamma(x)} B(x, a, v)$$

    The function $B$ is a valid value aggergator

    For example, if $f \leq g$, then
    %
    \begin{equation*}
        B((w,y), w', f) \leq B((w,y), w', g)
    \end{equation*}

\end{frame}


\begin{frame}
    
    Discuss possible generalizations

    \begin{itemize}
        \item state-dependent discounting
        \item recursive preferences
    \end{itemize}
     
\end{frame}


\begin{frame}
    \frametitle{Operators}

    Given $v$ in $\vV$, we call $\sigma \in \Sigma$ \navy{$v$-greedy}  if 
    %
    \begin{equation*}
        \sigma(x) \in \argmax_{a \in \Gamma(x)} B(x, a, v)
        \qquad \text{for all } x \in \Xsf
    \end{equation*}
    %

              \vspace{0.5em}
    The \navy{Bellman operator} is defined by
    %
    \begin{equation*}
        (T v)(x) = \max_{a \in \Gamma(x)} B(x, a, v)
        \qquad (x \in \Xsf, \; v \in \vV)
    \end{equation*}
    %

              \vspace{0.5em}

    \begin{itemize}
        \item $v^*$ solves the Bellman equation iff $v^*$ is a fixed point of $T$
    \end{itemize}


\end{frame}


\begin{frame}


    For each $\sigma \in \Sigma$, the \navy{policy operator} $T_\sigma$ is
    %
    \begin{equation*}
        (T_\sigma \, v)(x) = B(x, \sigma(x), v)
        \qquad (x \in \Xsf,\; v \in \vV)
    \end{equation*}
    %

    
        \vspace{0.5em}
        \vspace{0.5em}

    \Eg The policy operator for the savings problem, given $\sigma \in
    \Sigma$, is 
    %
    \begin{multline*}
        (T_\sigma \, v)(w, y) = 
        \\
        u(R w + y - \sigma(w, y))
            + \beta \sum_{y' \in \Ysf} v(\sigma(w, y), y') Q(y, y')
    \end{multline*}
    %

\end{frame}

\begin{frame}
    \frametitle{Stability}
    
    Let $\rR := (\Gamma, \vV, B)$ be an RDP with 
    %
    \begin{itemize}
        \item Bellman operator $T$ and 
        \item policy operators $\{T_\sigma\}_{\sigma \in \Sigma}$
    \end{itemize}


        \vspace{0.5em}
        \vspace{0.5em}
        \vspace{0.5em}
    We call $\rR$ \navy{globally stable} if
    %
    \begin{enumerate}
        \item $T$ is globally stable on $\vV$ and
        \vspace{0.5em}
        \item $T_\sigma$ is globally stable on $\vV$ for all $\sigma \in \Sigma$
    \end{enumerate}

\end{frame}


\begin{frame}
    
    \Eg In the inventory problem,


\end{frame}




\begin{frame}
    \frametitle{Lifetime Value}

    For a globally stable RDP, given $\sigma \in \Sigma$, let
    $v_\sigma$ be the unique solution to
    %
    \begin{equation*}
        v_\sigma(x) = B(x, \sigma(x), v_\sigma)
        \quad \text{for all } x \in \Xsf
    \end{equation*}
    %

    \begin{itemize}
        \item the unique fixed point of $T_\sigma$ 
    \end{itemize}


    \emp{Key idea:} $v_\sigma =$ lifetime value of following the
    policy $\sigma$ in each period 

    \Eg In the inventory problem, where
    %
    \begin{equation*}
        v_\sigma(x)
        = r(x, \sigma(x))
                + \beta
                \sum_d v_\sigma (F(x, \sigma(x), d) \phi(d)
    \end{equation*}

\end{frame}


\begin{frame}
    

\end{frame}

\end{document}









